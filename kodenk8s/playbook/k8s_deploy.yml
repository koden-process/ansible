---
# Playbook to deploy a Kubernetes cluster on 4 identical mini-PCs
# This playbook will:
# 1. Set up all nodes with prerequisites
# 2. Configure the first node as the control plane (master)
# 3. Join the other nodes as workers

# First, define host groups for control plane and workers
- name: Update inventory with control plane and worker groups
  hosts: localhost
  gather_facts: no
  tasks:
    - name: Add host groups
      add_host:
        name: "{{ item.name }}"
        groups: "{{ item.groups }}"
        ansible_host: "{{ hostvars[item.name]['ansible_host'] }}"
        ansible_user: "{{ hostvars[item.name]['ansible_user'] }}"
      loop:
        - { name: "koden0", groups: "k8s_control_plane" }
        - { name: "koden1", groups: "k8s_workers" }
        - { name: "koden2", groups: "k8s_workers" }
        - { name: "koden3", groups: "k8s_workers" }

# Common setup for all nodes
- name: Common Kubernetes setup for all nodes
  hosts: k8s_nodes
  become: true
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install prerequisites
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
          - software-properties-common
        state: present

    - name: Disable swap
      shell: swapoff -a
      changed_when: false

    - name: Remove swap from fstab
      replace:
        path: /etc/fstab
        regexp: '^([^#].*?\sswap\s+sw\s+.*)$'
        replace: '# \1'

    - name: Load required kernel modules
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter

    - name: Set up kernel modules to load at boot
      copy:
        dest: /etc/modules-load.d/k8s.conf
        content: |
          overlay
          br_netfilter
        mode: '0644'

    - name: Set up kernel parameters for Kubernetes
      copy:
        dest: /etc/sysctl.d/k8s.conf
        content: |
          net.bridge.bridge-nf-call-iptables = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward = 1
        mode: '0644'

    - name: Apply sysctl parameters
      command: sysctl --system
      changed_when: false

    # Install containerd
    - name: Create keyrings directory
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: Add Docker GPG key
      shell: |
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker-archive-keyring.gpg
      args:
        creates: /etc/apt/keyrings/docker-archive-keyring.gpg

    - name: Add Docker repository
      apt_repository:
        repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
        state: present

    - name: Install containerd
      apt:
        name: containerd.io
        state: present
        update_cache: yes

    - name: Create containerd config directory
      file:
        path: /etc/containerd
        state: directory
        mode: '0755'

    - name: Configure containerd
      shell: containerd config default > /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml

    - name: Update containerd configuration to use systemd cgroup driver
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'

    - name: Ensure containerd is configured to use CRI
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^disabled_plugins = \["cri"\]'
        state: absent

    - name: Restart containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes
        daemon_reload: yes

    # Install Kubernetes components
    - name: Add Kubernetes GPG key
      shell: |
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Add Kubernetes repository
      apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /"
        state: present
        filename: kubernetes

    - name: Install Kubernetes components
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes

    - name: Hold Kubernetes packages at their installed version
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: Create kubelet directory
      file:
        path: /var/lib/kubelet
        state: directory
        mode: '0755'

    - name: Create kubelet config file
      copy:
        dest: /var/lib/kubelet/config.yaml
        content: |
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          authentication:
            anonymous:
              enabled: false
            webhook:
              cacheTTL: 0s
              enabled: true
            x509:
              clientCAFile: /etc/kubernetes/pki/ca.crt
          authorization:
            mode: Webhook
          cgroupDriver: systemd
          clusterDNS:
          - 10.96.0.10
          clusterDomain: cluster.local
          containerRuntimeEndpoint: unix:///run/containerd/containerd.sock
          failSwapOn: false
          healthzBindAddress: 127.0.0.1
          healthzPort: 10248
          rotateCertificates: true
          runtimeRequestTimeout: 15m
          staticPodPath: /etc/kubernetes/manifests
        mode: '0644'

    - name: Configure kubelet environment variables
      copy:
        dest: /etc/default/kubelet
        content: |
          KUBELET_EXTRA_ARGS=--container-runtime-endpoint=unix:///run/containerd/containerd.sock
        mode: '0644'

    - name: Enable and start kubelet service
      systemd:
        name: kubelet
        state: started
        enabled: yes
        daemon_reload: yes

# Initialize the control plane
- name: Initialize Kubernetes control plane
  hosts: k8s_control_plane
  become: true
  tasks:
    - name: Initialize the Kubernetes cluster using kubeadm
      command: >
        kubeadm init --pod-network-cidr=192.168.0.0/16
        --apiserver-advertise-address={{ ansible_host }}
        --control-plane-endpoint={{ ansible_host }}
      register: kubeadm_init
      changed_when: kubeadm_init.rc == 0
      failed_when: kubeadm_init.rc != 0 and "already exists" not in kubeadm_init.stderr

    - name: Create .kube directory for the user
      file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'

    - name: Copy admin.conf to user's kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        remote_src: yes
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0644'

    - name: Ensure kubeconfig has correct server address
      replace:
        path: /home/{{ ansible_user }}/.kube/config
        regexp: 'server: https://[^:]+:6443'
        replace: 'server: https://{{ ansible_host }}:6443'

    - name: Wait for Kubernetes API to be available
      shell: |
        kubectl version --request-timeout='5s' --kubeconfig /etc/kubernetes/admin.conf
      register: kubeapi_check
      retries: 20
      delay: 10
      until: kubeapi_check.rc == 0

    - name: Generate kubeadm join command
      command: kubeadm token create --print-join-command
      register: join_command
      changed_when: false

    - name: Set join command fact
      set_fact:
        kubeadm_join_cmd: "{{ join_command.stdout }}"

    - name: Save join command to file for workers
      copy:
        content: "{{ join_command.stdout }}"
        dest: /tmp/kubeadm_join_cmd.sh
        mode: '0644'

    - name: Install Calico CNI
      shell: |
        kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: calico_apply
      until: calico_apply.rc == 0
      retries: 5
      delay: 10

# Join the workers to the cluster
- name: Join Kubernetes workers to the cluster
  hosts: k8s_workers
  become: true
  tasks:
    - name: Fetch join command from control-plane
      slurp:
        src: /tmp/kubeadm_join_cmd.sh
      register: join_cmd_file
      delegate_to: koden0

    - name: Run kubeadm join
      shell: "{{ join_cmd_file['content'] | b64decode }} --ignore-preflight-errors=all"
      args:
        creates: /etc/kubernetes/kubelet.conf

    - name: Ensure kubelet is started
      systemd:
        name: kubelet
        state: started
        enabled: yes
        daemon_reload: yes

# Verify the cluster
- name: Verify Kubernetes cluster
  hosts: k8s_control_plane
  become: false
  tasks:
    - name: Ensure KUBECONFIG environment variable is set
      shell: echo 'export KUBECONFIG=/home/{{ ansible_user }}/.kube/config' >> ~/.bashrc && source ~/.bashrc
      args:
        executable: /bin/bash
      changed_when: false

    - name: Verify kubectl configuration
      command: kubectl config view --minify
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      register: kubectl_config
      changed_when: false

    - name: Display kubectl configuration
      debug:
        var: kubectl_config.stdout_lines

    - name: Wait for all nodes to be ready
      command: kubectl wait --for=condition=Ready nodes --all --timeout=300s
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      register: nodes_ready
      changed_when: false
      failed_when: false

    - name: Get cluster status
      command: kubectl get nodes -o wide
      environment:
        KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
      register: cluster_status
      changed_when: false

    - name: Display cluster status
      debug:
        var: cluster_status.stdout_lines
